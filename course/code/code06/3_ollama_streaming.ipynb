{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff3a08ce-6986-4185-bed9-8ed81bb823d6",
   "metadata": {},
   "source": [
    "# Query Ollama based on REQUESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce9fb38e-fe0a-409d-b786-fb3b89ab16e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                  ID              SIZE      MODIFIED    \n",
      "deepseek-r1:latest    6995872bfe4c    5.2 GB    2 hours ago    \n",
      "mistral:latest        6577803aa9a0    4.4 GB    2 hours ago    \n",
      "mixtral:latest        a3b6bef0f836    26 GB     5 weeks ago    \n",
      "mixtral:8x7b          a3b6bef0f836    26 GB     5 weeks ago    \n",
      "llama3:latest         365c0bd3c000    4.7 GB    5 weeks ago    \n"
     ]
    }
   ],
   "source": [
    "# Make sure you have all models downloaded you want to use\n",
    "!ollama list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d911c166-5e9b-434c-bf74-01ba0d8bab04",
   "metadata": {},
   "source": [
    "## Streaming with text only, elementary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfa74ade-9899-4975-b37c-a4002582856f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Self-attention, also known as scaled dot-product attention or multi-head attention, is a key component of the Transformer model in natural language processing (NLP). It allows the model to focus on different parts of an input sequence when producing an output for each position. Here's a simplified explanation:\n",
      "\n",
      "1. Input Embeddings: Given an input sequence, each word or token is first converted into a dense vector representation through an embedding layer. These vectors are then processed further using positional encodings to maintain information about the position of tokens in the sequence.\n",
      "\n",
      "2. Splitting Inputs: After processing the initial embeddings, the input is split into three parts – query (Q), key (K), and value (V). The splitting allows for the calculation of attention scores across the input sequence.\n",
      "\n",
      "3. Attention Scores Calculation: To calculate attention scores between the query and keys, a dot product operation is performed. However, the raw dot products can lead to very large values, causing numerical instability, so they are divided by the square root of the dimension of the embeddings (which is the same for Q, K, and V). The resulting values represent the initial attention scores.\n",
      "\n",
      "4. Softmax Normalization: The attention scores are then passed through a softmax function, which converts the raw attention scores into probabilities that sum up to 1 for each query. These probabilities can be thought of as weights for the values in the sequence when producing an output for each position.\n",
      "\n",
      "5. Weighted Sum: The weighted sum of the value vectors (V) is calculated using the attention weights obtained from the softmax normalization step. This produces a context vector that provides information about the input sequence relevant to the query at hand.\n",
      "\n",
      "6. Outputs: Finally, the resulting context vectors are passed through one or more feed-forward neural networks to produce the final output for each position in the sequence.\n",
      "\n",
      "In practice, multi-head attention allows the model to process the input from multiple perspectives simultaneously by performing self-attention separately for different subspaces of the input space. This can help improve the model's ability to capture complex relationships within the input data."
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "data = {\n",
    "  \"model\": \"mistral\",\n",
    "  \"prompt\": \"Explain self-attention.\",\n",
    "  \"stream\": True }\n",
    "r = requests.post(url, json=data, stream=True)\n",
    "for line in r.iter_lines():\n",
    "    if line:\n",
    "        msg = json.loads(line)\n",
    "        print(msg[\"response\"], end=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3cb0730-8039-4a01-929e-f92faf15772b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Germany is Berlin.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def query_ollama(prompt, model=\"llama3\"):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False\n",
    "    }\n",
    "    answer = requests.post(url, json=data)\n",
    "    return answer.json()[\"response\"]\n",
    "\n",
    "# Example usage:\n",
    "print(query_ollama(\"What is the capital of Germany?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423487fd-8c9d-41f3-b317-48e32ab54e06",
   "metadata": {},
   "source": [
    "# Streaming response with Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c897a3a2-4f0f-427c-9253-521195b3197c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Self**-attention! A fundamental component of the **Transformer** architecture, revolutionizing the field of **Natural Language Processing** (**NLP**). **In** this explanation, I'll break down the concept of self-attention and its role within the **Transformer** model.\n",
       "\n",
       "\n",
       "\n",
       "****What** is self-attention?**\n",
       "\n",
       "\n",
       "\n",
       "**In** traditional recurrent neural networks (RNNs) and convolutional neural networks (CNNs), sequential relationships between input elements are modeled using recurrence or convolutional filters. **However**, these approaches have limitations when dealing with long-range dependencies, which are crucial in **NLP** tasks like language translation, question answering, and text summarization.\n",
       "\n",
       "\n",
       "\n",
       "**Self**-attention addresses this limitation by allowing the model to attend to different parts of the input sequence simultaneously, rather than relying on sequential processing. **In** essence, self-attention enables the model to weigh the importance of each input element relative to others, based on their relevance to the current task.\n",
       "\n",
       "\n",
       "\n",
       "****How** does self-attention work?**\n",
       "\n",
       "\n",
       "\n",
       "**The** self-attention mechanism is applied in parallel across all tokens (words or subwords) in a sequence. **The** process involves three main components:\n",
       "\n",
       "\n",
       "\n",
       "1. ****Query** (Q)**: **Each** token is associated with a query vector, which represents the token's context and its relationship to other tokens.\n",
       "\n",
       "2. ****Key** (K)**: **Each** token is also associated with a key vector, which captures the token's content and relevance to other tokens.\n",
       "\n",
       "3. ****Value** (V)**: **Each** token has a value vector, which contains the token's representation.\n",
       "\n",
       "\n",
       "\n",
       "**The** self-attention mechanism computes the attention weights by taking the dot product of each query and key vector, then applying a softmax function:\n",
       "\n",
       "\n",
       "\n",
       "`**Attention**(Q, K) = **Softmax**(Q * K / sqrt(d))`\n",
       "\n",
       "\n",
       "\n",
       "where `d` is the dimensionality of the vectors. **The** attention weights are computed for all tokens simultaneously, allowing the model to consider long-range dependencies.\n",
       "\n",
       "\n",
       "\n",
       "****What** happens next?**\n",
       "\n",
       "\n",
       "\n",
       "**The** output of self-attention is a weighted sum of the value vectors, where the weights are determined by the attention scores:\n",
       "\n",
       "\n",
       "\n",
       "`**Output** = **Concat**(head1, ..., headn) * WO`\n",
       "\n",
       "\n",
       "\n",
       "where `head1`, ..., `headn` are the outputs from multiple self-attention heads (more on this later). **The** output is then passed through a feed-forward neural network (**FFNN**) for further transformation.\n",
       "\n",
       "\n",
       "\n",
       "****Why** is self-attention important?**\n",
       "\n",
       "\n",
       "\n",
       "**Self**-attention enables **Transformers** to:\n",
       "\n",
       "\n",
       "\n",
       "* **Capture** long-range dependencies and contextual relationships between tokens\n",
       "\n",
       "* **Attend** to relevant parts of the input sequence, even if they are far apart in the original order\n",
       "\n",
       "* **Handle** complex **NLP** tasks that require understanding context and relationships\n",
       "\n",
       "\n",
       "\n",
       "**In** summary, self-attention is a critical component of the **Transformer** architecture, allowing it to attend to different parts of the input sequence simultaneously and capture long-range dependencies. **This** mechanism has had a profound impact on the field of **NLP**, enabling state-of-the-art results in many areas!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import re\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "\n",
    "def format_markdown(text):\n",
    "    text = text.replace(\"\\n\", \"\\n\\n\")\n",
    "    text = re.sub(r'\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*)\\b', r'**\\1**', text)\n",
    "    text = re.sub(r'\\b([A-Z]{3,})\\b', r'**\\1**', text)\n",
    "    return text\n",
    "\n",
    "def stream_ollama_markdown(prompt, model=\"llama3\"):\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    data = {\"model\": model, \"prompt\": prompt, \"stream\": True}\n",
    "\n",
    "    response = requests.post(url, json=data, stream=True)\n",
    "    buffer = \"\"\n",
    "    output_display = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "    for chunk in response.iter_lines():\n",
    "        if chunk:\n",
    "            try:\n",
    "                data = json.loads(chunk.decode(\"utf-8\"))\n",
    "                text = data.get(\"response\", \"\")\n",
    "                if text:\n",
    "                    buffer += text\n",
    "                    clear_output(wait=True)\n",
    "                    output_display.update(Markdown(format_markdown(buffer)))\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "\n",
    "    # Final display (in case last chunk isn’t shown)\n",
    "    clear_output(wait=True)\n",
    "    output_display.update(Markdown(format_markdown(buffer)))\n",
    "    # No return\n",
    "\n",
    "\n",
    "# Try it\n",
    "stream_ollama_markdown(\"Explain the concept of self-attention in Transformers.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fc3734-ec91-4e83-b89e-9627125b675f",
   "metadata": {},
   "source": [
    "# Preparing python based ollama access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db9675a7-85a4-4c52-bd9e-6e89d4c8b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.environ[\"http_proxy\"]  = \"http://ofsquid.dwd.de:8080\"\n",
    "#os.environ[\"https_proxy\"] = \"http://ofsquid.dwd.de:8080\"\n",
    "#os.environ[\"no_proxy\"]    = \"localhost,127.0.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab0fdf83-22a8-4441-ae1c-78518f44d3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6552764b-10e8-463d-bb28-8528cf3f9e9a",
   "metadata": {},
   "source": [
    "## First example, querying ollama, no streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "585ff4cc-1c01-4618-8f42-f8d561c9dd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model='llama3', messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}])\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f17cb3-be8d-4063-994d-83d27ab47ce7",
   "metadata": {},
   "source": [
    "## Token Speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05baa68c-936f-441d-ba02-00853a4bce4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏳ Testing model: mistral\n",
      "⏳ Testing model: llama3\n",
      "⏳ Testing model: deepseek-r1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import os\n",
    "\n",
    "models = [\n",
    "    \"mistral\",\n",
    "    \"llama3\",\n",
    "    \"deepseek-r1\",\n",
    "]\n",
    "\n",
    "prompt = \"Explain self-attention in two paragraphs.\"\n",
    "page_token_count = 333  # ≈ one page\n",
    "\n",
    "results = []    \n",
    "for model in models:\n",
    "    print(f\"⏳ Testing model: {model}\")\n",
    "    start = time.time()\n",
    "    response = query_ollama(prompt, model)\n",
    "    end = time.time()\n",
    "\n",
    "    tokens = len(response.split())\n",
    "    duration = end - start\n",
    "    tokens_per_sec = tokens / duration\n",
    "    seconds_per_page = page_token_count / tokens_per_sec\n",
    "\n",
    "    results.append({\n",
    "        \"model\": model,\n",
    "        \"tokens\": tokens,\n",
    "        \"duration_sec\": duration,\n",
    "        \"tokens/sec\": tokens_per_sec,\n",
    "        \"sec/page\": seconds_per_page\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db7d328a-270c-4dc4-bdcb-f93db1d2ab91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>tokens</th>\n",
       "      <th>duration_sec</th>\n",
       "      <th>tokens/sec</th>\n",
       "      <th>sec/page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mistral</td>\n",
       "      <td>180</td>\n",
       "      <td>5.06</td>\n",
       "      <td>35.54</td>\n",
       "      <td>9.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>llama3</td>\n",
       "      <td>241</td>\n",
       "      <td>6.81</td>\n",
       "      <td>35.41</td>\n",
       "      <td>9.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deepseek-r1</td>\n",
       "      <td>240</td>\n",
       "      <td>16.26</td>\n",
       "      <td>14.76</td>\n",
       "      <td>22.57</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         model  tokens  duration_sec  tokens/sec  sec/page\n",
       "0      mistral     180          5.06       35.54      9.37\n",
       "1       llama3     241          6.81       35.41      9.41\n",
       "2  deepseek-r1     240         16.26       14.76     22.57"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display results nicely\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "df = df.round(2)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c017925-f79f-4e59-99b7-5c250e756244",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
