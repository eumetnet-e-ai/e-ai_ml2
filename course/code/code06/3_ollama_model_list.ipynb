{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3deef3f5-9bab-4fcd-ae25-22dea96ff6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"http_proxy\"]  = \"http://ofsquid.dwd.de:8080\"\n",
    "os.environ[\"https_proxy\"] = \"http://ofsquid.dwd.de:8080\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a869a4ae-24d2-485d-84f3-3629865a9c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# URL of the Ollama model library\n",
    "OLLAMA_MODEL_LIBRARY_URL = \"https://ollama.com/library\"\n",
    "\n",
    "def fetch_available_models():\n",
    "    \"\"\"Scrapes the Ollama model library and correctly pairs models with their parameter sizes.\"\"\"\n",
    "    response = requests.get(OLLAMA_MODEL_LIBRARY_URL)\n",
    "    if response.status_code != 200:\n",
    "        print(\"‚ùå Failed to fetch model list from Ollama.\")\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    models = []\n",
    "\n",
    "    # Find all links containing model names (/library/{model_name})\n",
    "    for link in soup.find_all(\"a\", href=True):\n",
    "        if link[\"href\"].startswith(\"/library/\"):\n",
    "            model_name = link[\"href\"].split(\"/\")[-1]  # Extract model name from URL\n",
    "\n",
    "            # Look for size in the same block (search text within the same section)\n",
    "            parent = link.find_parent()\n",
    "            size_text = \"Unknown\"\n",
    "            if parent:\n",
    "                text_in_block = parent.get_text(\" \", strip=True)\n",
    "                size_matches = re.findall(r\"(\\d+(\\.\\d+)?b)\", text_in_block, re.IGNORECASE)\n",
    "                sizes = [s[0] for s in size_matches]  # Extract sizes like \"7b\", \"70b\", etc.\n",
    "\n",
    "                # Store the largest size or all available ones\n",
    "                size_text = \", \".join(sizes) if sizes else \"Unknown\"\n",
    "\n",
    "            models.append((model_name, size_text))\n",
    "\n",
    "    return models\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcc2a961-7ee4-47ad-802c-2e258008a4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Fetching available models from the Ollama model library...\n",
      "\n",
      "üìå Available Ollama Models:\n",
      "1) nemotron-3-nano - 30b, \n",
      "2) functiongemma - Unknown, \n",
      "3) olmo-3 - 7b, 32b, \n",
      "4) gemini-3-flash-preview - Unknown, \n",
      "5) devstral-small-2 - 24B, 24b, \n",
      "6) devstral-2 - 123B, 123b, \n",
      "7) ministral-3 - 3b, 8b, 14b, \n",
      "8) qwen3-vl - 2b, 4b, 8b, 30b, 32b, 235b, \n",
      "9) gpt-oss - 20b, 120b, \n",
      "10) deepseek-r1 - 1.5b, 7b, 8b, 14b, 32b, 70b, 671b, \n",
      "11) qwen3-coder - 30b, 480b, \n",
      "12) gemma3 - 1b, 4b, 12b, 27b, \n",
      "13) llama3.1 - 8B, 70B, 405B, 8b, 70b, 405b, \n",
      "14) llama3.2 - 1B, 3B, 1b, 3b, \n",
      "15) nomic-embed-text - Unknown, \n",
      "16) mistral - 7B, 7b, \n",
      "17) qwen2.5 - 0.5b, 1.5b, 3b, 7b, 14b, 32b, 72b, \n",
      "18) qwen3 - 0.6b, 1.7b, 4b, 8b, 14b, 30b, 32b, 235b, \n",
      "19) phi3 - 3B, 14B, 3.8b, 14b, \n",
      "20) llama3 - 8b, 70b, \n",
      "21) llava - 7b, 13b, 34b, \n",
      "22) gemma2 - 2B, 9B, 27B, 2b, 9b, 27b, \n",
      "23) qwen2.5-coder - 0.5b, 1.5b, 3b, 7b, 14b, 32b, \n",
      "24) phi4 - 14B, 14b, \n",
      "25) mxbai-embed-large - Unknown, \n",
      "26) gemma - 2b, 7b, \n",
      "27) qwen - 0.5B, 110B, 0.5b, 1.8b, 4b, 7b, 14b, 32b, 72b, 110b, \n",
      "28) llama2 - 7B, 70B, 7b, 13b, 70b, \n",
      "29) qwen2 - 0.5b, 1.5b, 7b, 72b, \n",
      "30) minicpm-v - 8b, \n",
      "31) codellama - 7b, 13b, 34b, 70b, \n",
      "32) dolphin3 - 8B, 8b, \n",
      "33) olmo2 - 7B, 13B, 7b, 13b, \n",
      "34) llama3.2-vision - 11B, 90B, 11b, 90b, \n",
      "35) tinyllama - 1.1B, 1.1b, \n",
      "36) mistral-nemo - 12B, 12b, \n",
      "37) deepseek-v3 - 671B, 37B, 671b, \n",
      "38) bge-m3 - Unknown, \n",
      "39) llama3.3 - 70B, 70B, 405B, 70b, \n",
      "40) deepseek-coder - 1.3b, 6.7b, 33b, \n",
      "41) smollm2 - 1.7B, 1.7b, \n",
      "42) mistral-small - 70B, 22b, 24b, \n",
      "43) all-minilm - Unknown, \n",
      "44) llava-llama3 - 8b, \n",
      "45) qwq - 32b, \n",
      "46) codegemma - 2b, 7b, \n",
      "47) falcon3 - 10B, 1b, 3b, 7b, 10b, \n",
      "48) starcoder2 - 3B, 7B, 15B, 3b, 7b, 15b, \n",
      "49) granite3.1-moe - 1B, 3B, 1b, 3b, \n",
      "50) mixtral - 7b, 22b, 7b, 22b, \n",
      "51) llama2-uncensored - 7b, 70b, \n",
      "52) snowflake-arctic-embed - Unknown, \n",
      "53) orca-mini - 3b, 7b, 13b, 70b, \n",
      "54) deepseek-coder-v2 - 16b, 236b, \n",
      "55) qwen2.5vl - 3b, 7b, 32b, 72b, \n",
      "56) cogito - 3b, 8b, 14b, 32b, 70b, \n",
      "57) mistral-small3.2 - 24b, \n",
      "58) gemma3n - 2b, 4b, \n",
      "59) llama4 - 17b, 17b, \n",
      "60) deepscaler - 1.5B, 1.5B, 1.5b, \n",
      "61) dolphin-phi - 2.7B, 2.7b, \n",
      "62) phi4-reasoning - 14b, \n",
      "63) magistral - 24B, 24b, \n",
      "64) phi - 2.7B, 2.7b, \n",
      "65) dolphin-mixtral - 7b, 22b, 7b, 22b, \n",
      "66) granite3.3 - 2B, 8B, 2b, 8b, \n",
      "67) phi4-mini - 3.8b, \n",
      "68) dolphin-llama3 - 8B, 70B, 8b, 70b, \n",
      "69) openthinker - 7b, 32b, \n",
      "70) codestral - 22b, \n",
      "71) smollm - 1.7B, 1.7b, \n",
      "72) granite3.2-vision - 2b, \n",
      "73) devstral - 24b, \n",
      "74) wizardlm2 - 7b, 22b, \n",
      "75) dolphin-mistral - 7b, \n",
      "76) deepcoder - 14B, 1.5B, 1.5b, 14b, \n",
      "77) moondream - 1.8b, \n",
      "78) mistral-small3.1 - 24b, \n",
      "79) command-r - 35b, \n",
      "80) granite-code - 3b, 8b, 20b, 34b, \n",
      "81) phi3.5 - 3.8b, \n",
      "82) hermes3 - 3b, 8b, 70b, 405b, \n",
      "83) bakllava - 7B, 7b, \n",
      "84) granite4 - 1b, 3b, \n",
      "85) yi - 6b, 9b, 34b, \n",
      "86) embeddinggemma - Unknown, \n",
      "87) zephyr - 7b, 141b, \n",
      "88) exaone-deep - 2.4B, 32B, 2.4b, 7.8b, 32b, \n",
      "89) mistral-large - 123b, \n",
      "90) wizard-vicuna-uncensored - 7B, 13B, 30B, 7b, 13b, 30b, \n",
      "91) opencoder - 1.5B, 8B, 1.5b, 8b, \n",
      "92) starcoder - 1b, 3b, 7b, 15b, \n",
      "93) nous-hermes - 7b, 13b, \n",
      "94) falcon - 7b, 40b, 180b, \n",
      "95) deepseek-llm - 7b, 67b, \n",
      "96) openchat - 7b, \n",
      "97) vicuna - 7b, 13b, 33b, \n",
      "98) deepseek-v2 - 16b, 236b, \n",
      "99) openhermes - 7B, \n",
      "100) codeqwen - 7b, \n",
      "101) deepseek-v3.1 - 671b, \n",
      "102) codegeex4 - 9b, \n",
      "103) mistral-openorca - 7B, 7b, \n",
      "104) command-r-plus - 104b, \n",
      "105) qwen3-next - 80b, \n",
      "106) qwen2-math - 1.5b, 7b, 72b, \n",
      "107) qwen3-embedding - 0.6b, 4b, 8b, \n",
      "108) paraphrase-multilingual - Unknown, \n",
      "109) snowflake-arctic-embed2 - Unknown, \n",
      "110) glm4 - 9b, \n",
      "111) aya - 8b, 35b, \n",
      "112) llama2-chinese - 7b, 13b, \n",
      "113) tinydolphin - 1.1B, 1.1b, \n",
      "114) stable-code - 3B, 7B, 3b, \n",
      "115) granite3.2 - 2b, 8b, \n",
      "116) neural-chat - 7b, \n",
      "117) nous-hermes2 - 10.7b, 34b, \n",
      "118) wizardcoder - 33b, \n",
      "119) sqlcoder - 7b, 15b, \n",
      "120) stablelm2 - 1.6B, 12B, 1.6b, 12b, \n",
      "121) yi-coder - 1.5b, 9b, \n",
      "122) llama3-chatqa - 8b, 70b, \n",
      "123) granite3-dense - 2B, 8B, 2b, 8b, \n",
      "124) granite3.1-dense - 2B, 8B, 2b, 8b, \n",
      "125) bge-large - Unknown, \n",
      "126) r1-1776 - 70b, 671b, \n",
      "127) wizard-math - 7b, 13b, 70b, \n",
      "128) llava-phi3 - 3.8b, \n",
      "129) llama3-gradient - 8B, 8b, 70b, \n",
      "130) dolphincoder - 7B, 15B, 7b, 15b, \n",
      "131) samantha-mistral - 7b, \n",
      "132) exaone3.5 - 2.4B, 32B, 2.4b, 7.8b, 32b, \n",
      "133) reflection - 70b, \n",
      "134) granite-embedding - Unknown, \n",
      "135) internlm2 - 7B, 1.8b, 7b, 20b, \n",
      "136) nemotron-mini - 4b, \n",
      "137) starling-lm - 7b, \n",
      "138) llama3-groq-tool-use - 8b, 70b, \n",
      "139) phind-codellama - 34b, \n",
      "140) solar - 10.7B, 10.7b, \n",
      "141) xwinlm - 7b, 13b, \n",
      "142) llama-guard3 - 1b, 8b, \n",
      "143) dbrx - 132b, \n",
      "144) athene-v2 - 72B, 72b, \n",
      "145) tulu3 - 8b, 70b, \n",
      "146) aya-expanse - 8b, 32b, \n",
      "147) yarn-llama2 - 7b, 13b, \n",
      "148) granite3-moe - 1B, 3B, 1b, 3b, \n",
      "149) meditron - 7b, 70b, \n",
      "150) nemotron - 70B, 70b, \n",
      "151) wizardlm-uncensored - 13b, \n",
      "152) orca2 - 7b, 13b, \n",
      "153) stable-beluga - 7b, 13b, 70b, \n",
      "154) medllama2 - 7b, \n",
      "155) smallthinker - 3B, 3b, \n",
      "156) nous-hermes2-mixtral - 7b, \n",
      "157) reader-lm - 0.5b, 1.5b, \n",
      "158) command-r7b - 7b, 7b, \n",
      "159) phi4-mini-reasoning - 3.8b, \n",
      "160) shieldgemma - 2b, 9b, 27b, \n",
      "161) llama-pro - Unknown, \n",
      "162) deepseek-v2.5 - 236b, \n",
      "163) yarn-mistral - 7b, \n",
      "164) wizardlm - Unknown, \n",
      "165) nexusraven - 13B, 13b, \n",
      "166) mathstral - 7B, 7b, \n",
      "167) everythinglm - 13b, \n",
      "168) codeup - 13b, \n",
      "169) command-a - 111b, \n",
      "170) stablelm-zephyr - 3b, \n",
      "171) falcon2 - 11B, 11b, \n",
      "172) solar-pro - 22b, \n",
      "173) duckdb-nsql - 7B, 7b, \n",
      "174) magicoder - 7B, 7b, \n",
      "175) mistrallite - 7b, \n",
      "176) marco-o1 - 7b, \n",
      "177) bespoke-minicheck - 7b, \n",
      "178) codebooga - 34b, \n",
      "179) nuextract - 3.8B, 3.8b, \n",
      "180) deepseek-ocr - 3b, \n",
      "181) wizard-vicuna - 13B, 13b, \n",
      "182) firefunction-v2 - 70b, \n",
      "183) megadolphin - 120b, 70b, 120b, \n",
      "184) notux - 7b, \n",
      "185) granite3-guardian - 2B, 8B, 2b, 8b, \n",
      "186) open-orca-platypus2 - 13b, \n",
      "187) notus - 7B, 7b, \n",
      "188) sailor2 - 1B, 8B, 20B, 1b, 8b, 20b, \n",
      "189) goliath - 70B, \n",
      "190) alfred - 40b, \n",
      "191) gemini-3-pro-preview - Unknown, \n",
      "192) command-r7b-arabic - 7b, 7B, 7b, \n",
      "193) glm-4.6 - Unknown, \n",
      "194) gpt-oss-safeguard - 20b, 120b, 20b, 120b, \n",
      "195) minimax-m2 - Unknown, \n",
      "196) cogito-2.1 - 671b, \n",
      "197) kimi-k2 - Unknown, \n",
      "198) rnj-1 - 8B, 8b, \n",
      "199) kimi-k2-thinking - Unknown, \n",
      "200) olmo-3.1 - 32b, \n",
      "201) nomic-embed-text-v2-moe - Unknown, \n",
      "202) deepseek-v3.2 - Unknown, \n",
      "203) mistral-large-3 - Unknown, \n",
      "204) glm-4.7 - Unknown, \n",
      "205) minimax-m2.1 - Unknown, \n"
     ]
    }
   ],
   "source": [
    "print(\"üîç Fetching available models from the Ollama model library...\")\n",
    "models = fetch_available_models()\n",
    "\n",
    "print(\"\\nüìå Available Ollama Models:\")\n",
    "for i, (name, size) in enumerate(models, 1):\n",
    "    print(f\"{i}) {name} - {size}\", end=', \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b55eaa9e-9b25-4aa7-982d-c626818e4ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 80, Test dataset size: 20\n",
      "First training sample - X shape: torch.Size([10]), y shape: torch.Size([1])\n",
      "First training sample - X: [0.82811207 0.73099226 0.6460811  0.98673284 0.5317019  0.11775014\n",
      " 0.99185634 0.32278016 0.5298668  0.3955642 ], y: [0.8041298]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Generate synthetic data: 100 samples with 10 features each\n",
    "X = np.random.rand(100, 10)\n",
    "y = np.random.rand(100, 1)\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Create a TensorDataset and then split it into training and test sets\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# Diagnostic Output\n",
    "print(f\"Train dataset size: {len(train_dataset)}, Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Show shapes of the first batch\n",
    "first_train_sample = train_dataset[0]\n",
    "print(f\"First training sample - X shape: {first_train_sample[0].shape}, y shape: {first_train_sample[1].shape}\")\n",
    "\n",
    "# Show content of the first training sample\n",
    "print(f\"First training sample - X: {first_train_sample[0].numpy()}, y: {first_train_sample[1].numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddce1a6b-0813-49ca-b34c-4dc4ded4bae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=10, out_features=16, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (fc2): Linear(in_features=16, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "input_size = 10\n",
    "hidden_size = 16\n",
    "output_size = 1\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b5fe747-6462-4adc-af5b-ff16d9920aa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Batch X shape: torch.Size([16, 10])\n",
      "      Batch y shape: torch.Size([16, 1])\n",
      "2) Batch X shape: torch.Size([16, 10])\n",
      "      Batch y shape: torch.Size([16, 1])\n",
      "3) Batch X shape: torch.Size([16, 10])\n",
      "      Batch y shape: torch.Size([16, 1])\n",
      "4) Batch X shape: torch.Size([16, 10])\n",
      "      Batch y shape: torch.Size([16, 1])\n",
      "5) Batch X shape: torch.Size([16, 10])\n",
      "      Batch y shape: torch.Size([16, 1])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Create a DataLoader for the training dataset\n",
    "dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Example: Iterate through one batch\n",
    "n = 1\n",
    "for batch_X, batch_y in dataloader:\n",
    "    print(f\"{n}) Batch X shape:\", batch_X.size())\n",
    "    print(\"      Batch y shape:\", batch_y.size())\n",
    "    n += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "033eecfa-5c1a-4bac-be25-d16fe0fb46cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf78a62-4f58-45ce-9467-b4b2f66ef00c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
