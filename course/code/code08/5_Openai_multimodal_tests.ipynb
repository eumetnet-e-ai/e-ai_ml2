{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fb6d6ed-ac4d-47d7-9c9e-8d779ed096d4",
   "metadata": {},
   "source": [
    "# OpenAI Model Discovery and Multimodal Capability Check\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "\n",
    "1. Configure network access (proxy)\n",
    "2. Load an OpenAI API key from a `.env` file\n",
    "3. Query all OpenAI models available to the account\n",
    "4. Programmatically test which models accept image inputs\n",
    "\n",
    "The goal is **inspection and understanding**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c36cb60-2266-4168-a09a-1930be31b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# DWD proxy\n",
    "os.environ[\"HTTP_PROXY\"]  = \"http://ofsquid.dwd.de:8080\"\n",
    "os.environ[\"HTTPS_PROXY\"] = \"http://ofsquid.dwd.de:8080\"\n",
    "\n",
    "# Optional but recommended\n",
    "os.environ[\"http_proxy\"]  = os.environ[\"HTTP_PROXY\"]\n",
    "os.environ[\"https_proxy\"] = os.environ[\"HTTPS_PROXY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "904c0017-754f-4ecc-85e7-98053c813df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded: True\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "# Create OpenAI client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "print(\"API key loaded:\", bool(os.getenv(\"OPENAI_API_KEY\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb0cf024-992a-4ccf-b5b2-26f0d896ac1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "\n",
      " - gpt-4-0613\n",
      " - gpt-4\n",
      " - gpt-3.5-turbo\n",
      " - chatgpt-image-latest\n",
      " - gpt-4o-mini-tts-2025-03-20\n",
      " - gpt-4o-mini-tts-2025-12-15\n",
      " - gpt-realtime-mini-2025-12-15\n",
      " - gpt-audio-mini-2025-12-15\n",
      " - davinci-002\n",
      " - babbage-002\n",
      " - gpt-3.5-turbo-instruct\n",
      " - gpt-3.5-turbo-instruct-0914\n",
      " - dall-e-3\n",
      " - dall-e-2\n",
      " - gpt-4-1106-preview\n",
      " - gpt-3.5-turbo-1106\n",
      " - tts-1-hd\n",
      " - tts-1-1106\n",
      " - tts-1-hd-1106\n",
      " - text-embedding-3-small\n",
      " - text-embedding-3-large\n",
      " - gpt-4-0125-preview\n",
      " - gpt-4-turbo-preview\n",
      " - gpt-3.5-turbo-0125\n",
      " - gpt-4-turbo\n",
      " - gpt-4-turbo-2024-04-09\n",
      " - gpt-4o\n",
      " - gpt-4o-2024-05-13\n",
      " - gpt-4o-mini-2024-07-18\n",
      " - gpt-4o-mini\n",
      " - gpt-4o-2024-08-06\n",
      " - chatgpt-4o-latest\n",
      " - gpt-4o-audio-preview\n",
      " - gpt-4o-realtime-preview\n",
      " - omni-moderation-latest\n",
      " - omni-moderation-2024-09-26\n",
      " - gpt-4o-realtime-preview-2024-12-17\n",
      " - gpt-4o-audio-preview-2024-12-17\n",
      " - gpt-4o-mini-realtime-preview-2024-12-17\n",
      " - gpt-4o-mini-audio-preview-2024-12-17\n",
      " - o1-2024-12-17\n",
      " - o1\n",
      " - gpt-4o-mini-realtime-preview\n",
      " - gpt-4o-mini-audio-preview\n",
      " - o3-mini\n",
      " - o3-mini-2025-01-31\n",
      " - gpt-4o-2024-11-20\n",
      " - gpt-4o-search-preview-2025-03-11\n",
      " - gpt-4o-search-preview\n",
      " - gpt-4o-mini-search-preview-2025-03-11\n",
      " - gpt-4o-mini-search-preview\n",
      " - gpt-4o-transcribe\n",
      " - gpt-4o-mini-transcribe\n",
      " - o1-pro-2025-03-19\n",
      " - o1-pro\n",
      " - gpt-4o-mini-tts\n",
      " - o3-2025-04-16\n",
      " - o4-mini-2025-04-16\n",
      " - o3\n",
      " - o4-mini\n",
      " - gpt-4.1-2025-04-14\n",
      " - gpt-4.1\n",
      " - gpt-4.1-mini-2025-04-14\n",
      " - gpt-4.1-mini\n",
      " - gpt-4.1-nano-2025-04-14\n",
      " - gpt-4.1-nano\n",
      " - gpt-image-1\n",
      " - codex-mini-latest\n",
      " - o3-pro\n",
      " - gpt-4o-realtime-preview-2025-06-03\n",
      " - gpt-4o-audio-preview-2025-06-03\n",
      " - o3-pro-2025-06-10\n",
      " - o4-mini-deep-research\n",
      " - o3-deep-research\n",
      " - gpt-4o-transcribe-diarize\n",
      " - o3-deep-research-2025-06-26\n",
      " - o4-mini-deep-research-2025-06-26\n",
      " - gpt-5-chat-latest\n",
      " - gpt-5-2025-08-07\n",
      " - gpt-5\n",
      " - gpt-5-mini-2025-08-07\n",
      " - gpt-5-mini\n",
      " - gpt-5-nano-2025-08-07\n",
      " - gpt-5-nano\n",
      " - gpt-audio-2025-08-28\n",
      " - gpt-realtime\n",
      " - gpt-realtime-2025-08-28\n",
      " - gpt-audio\n",
      " - gpt-5-codex\n",
      " - gpt-image-1-mini\n",
      " - gpt-5-pro-2025-10-06\n",
      " - gpt-5-pro\n",
      " - gpt-audio-mini\n",
      " - gpt-audio-mini-2025-10-06\n",
      " - gpt-5-search-api\n",
      " - gpt-realtime-mini\n",
      " - gpt-realtime-mini-2025-10-06\n",
      " - sora-2\n",
      " - sora-2-pro\n",
      " - gpt-5-search-api-2025-10-14\n",
      " - gpt-5.1-chat-latest\n",
      " - gpt-5.1-2025-11-13\n",
      " - gpt-5.1\n",
      " - gpt-5.1-codex\n",
      " - gpt-5.1-codex-mini\n",
      " - gpt-5.1-codex-max\n",
      " - gpt-image-1.5\n",
      " - gpt-5.2-2025-12-11\n",
      " - gpt-5.2\n",
      " - gpt-5.2-pro-2025-12-11\n",
      " - gpt-5.2-pro\n",
      " - gpt-5.2-chat-latest\n",
      " - gpt-4o-mini-transcribe-2025-12-15\n",
      " - gpt-4o-mini-transcribe-2025-03-20\n",
      " - gpt-3.5-turbo-16k\n",
      " - tts-1\n",
      " - whisper-1\n",
      " - text-embedding-ada-002\n"
     ]
    }
   ],
   "source": [
    "# List all models accessible to this OpenAI account\n",
    "models = client.models.list()\n",
    "\n",
    "print(\"Available models:\\n\")\n",
    "for m in models.data:\n",
    "    print(\" -\", m.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec5977-0f92-4eee-8a13-eb366393216a",
   "metadata": {},
   "source": [
    "### Loading and Encoding an Image for Multimodal Input\n",
    "\n",
    "OpenAI multimodal models expect images to be provided as URLs or\n",
    "base64-encoded image data.\n",
    "\n",
    "In this notebook, the radar reflectivity image is:\n",
    "1. loaded from disk,\n",
    "2. encoded as base64,\n",
    "3. embedded into a data URL (`data:image/png;base64,...`).\n",
    "\n",
    "This representation can be passed directly to vision-enabled models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d10f86f7-b4f1-45ce-8ba9-b7f3d0f7dafd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image successfully loaded and encoded.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "\n",
    "# Path to the image used for multimodal testing\n",
    "image_path = \"radar_map_germany.png\"\n",
    "\n",
    "# Read image and encode as base64 string\n",
    "with open(image_path, \"rb\") as f:\n",
    "    image_b64 = base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "print(\"Image successfully loaded and encoded.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e716e7-6be4-478f-9bd9-6a1c3fd77c71",
   "metadata": {},
   "source": [
    "### Testing Multimodal Capabilities of OpenAI Models\n",
    "\n",
    "The following function tests whether a given OpenAI model can process image inputs.\n",
    "\n",
    "It sends a minimal multimodal request consisting of:\n",
    "- a short text prompt, and\n",
    "- a base64-encoded PNG image.\n",
    "\n",
    "If the model supports image input, it returns a textual description of the image.\n",
    "If not, the request fails and the exception is caught.\n",
    "\n",
    "This approach is used only for **demonstration and inspection**, not for\n",
    "automatic capability discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6c8f520b-8e31-4a09-a658-3cb82d9ad8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model_name, image_b64):\n",
    "    \"\"\"\n",
    "    Test whether a given OpenAI model supports image input.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str\n",
    "        Name of the OpenAI model to test.\n",
    "    image_b64 : str\n",
    "        Base64-encoded PNG image.\n",
    "\n",
    "    The function sends a minimal multimodal request consisting of:\n",
    "    - a short text instruction\n",
    "    - an image passed via image_url\n",
    "\n",
    "    If the model supports vision input, a textual description is printed.\n",
    "    If not, the API raises an error which is caught and reported.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model_name,\n",
    "            messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\",\n",
    "                     \"text\": \"Describe this image briefly.\"},\n",
    "                    {\"type\": \"image_url\",\n",
    "                     \"image_url\": {\n",
    "                         \"url\": f\"data:image/png;base64,{image_b64}\"\n",
    "                     }}\n",
    "                ]\n",
    "            }],\n",
    "            max_tokens=120,\n",
    "        )\n",
    "\n",
    "        print(f\"\\n[{model_name}]\")\n",
    "        print(response.choices[0].message.content)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n[{model_name}] FAILED:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0399adbb-ee81-42e8-a248-1fbc1f9ba798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[gpt-5-chat-latest]\n",
      "This image is a weather radar map titled “DWD Radar Reflectivity with State Borders.” It shows radar reflectivity data over Central Europe, including Germany and surrounding countries. Different colors represent varying levels of reflectivity (in dBZ), with the color scale on the right ranging from 0 (dark blue) to 40 (dark red). The areas with green, yellow, and red colors indicate stronger radar returns, likely corresponding to regions of heavier precipitation or storms.\n"
     ]
    }
   ],
   "source": [
    "# Run function on particular model\n",
    "test_model(\"gpt-5-chat-latest\", image_b64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d96c17a-1d56-4c07-a19d-d2ecd649bb91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[gpt-3.5-turbo] FAILED: Error code: 400 - {'error': {'message': 'Invalid content type. image_url is only supported by certain models.', 'type': 'invalid_request_error', 'param': 'messages.[0].content.[1].type', 'code': None}}\n"
     ]
    }
   ],
   "source": [
    "# Here a model which does not support image input\n",
    "test_model(\"gpt-3.5-turbo\", image_b64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ebd46b-9ecf-4d3b-b543-e9d423c7ef99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(ropy)",
   "language": "python",
   "name": "ropy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
