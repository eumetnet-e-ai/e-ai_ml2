%!TEX root = lec16.tex
% ================================================================================
% Lecture 16 â€” Slide 29
% ================================================================================
\begin{frame}[t]
\mytitle{What the Neural Network Sees}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.5\textwidth}
\footnotesize
\vspace{-2mm}

\textbf{Important: the input is the \y{full signal}, not single points.}

\vspace{1mm}
The network gets all grid values of $f(t,x)$ at once:
\[
f(t,\cdot)\in\mathbb{R}^{N}\qquad (N=256)
\]

\vspace{2mm}
\textbf{But the computation is local:}
each output point is computed from a \y{neighborhood}.

\vspace{6mm}
\textbf{Kernel size $k$ = width of the filter}

\vspace{1mm}
With $k=7$, the network looks at:
\[
[x_{i-3},\dots,x_i,\dots,x_{i+3}]
\]

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.44\textwidth}

\vspace{-6mm}
\textbf{Stacking layers increases context}

\vspace{1mm}
After 3 conv layers (all $k=7$), each output point depends on roughly:
\[
1 + 3\cdot(k-1) = 19\ \text{grid points}
\]

\hspace*{-1cm}\includegraphics[height=4cm]{../../images/img16/cnn_layers_influence.png}

\end{column}

\end{columns}
\end{frame}
