%!TEX root = lec11.tex
% ================================================================================
% Lecture 11 â€” Slide 10
% ================================================================================
\begin{frame}[t,fragile]
\begin{tightmath}

\mytitle{LLM Streaming Routes}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.48\textwidth}

\textbf{Purpose of LLM Routes}

\begin{itemize}
  \item Accept user queries from the gateway
  \item Build prompts from session history
  \item Integrate retrieved context (RAG)
  \item Return responses as \y{streams}
\end{itemize}

\vspace{2mm}
\textbf{Streaming Logic}

\begin{itemize}
  \item Token-wise or chunk-wise output
  \item Immediate forwarding to frontend
  \item Non-blocking async execution
\end{itemize}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.5\textwidth}

\vspace{-8mm}
\textbf{Unified Interface}

\begin{itemize}
  \item Same route for local and cloud models
  \item OpenAI, Claude, Gemini, LLaMA, etc.
  \item Model chosen at runtime
\end{itemize}

\vspace{2mm}
\textbf{Interception Points}

\begin{itemize}
  \item Function call detection
  \item Tool execution triggers
  \item Session state updates
\end{itemize}

\vspace{2mm}
\textbf{Key Insight}

\begin{itemize}
  \item LLM output is \rtext{not final}
  \item It is an intermediate signal
\end{itemize}

\end{column}

\end{columns}

\end{tightmath}
\end{frame}
