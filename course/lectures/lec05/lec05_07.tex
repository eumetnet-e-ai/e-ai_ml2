%!TEX root = lec05.tex
% ================================================================================
% Lecture 5 â€” Slide 7
% ================================================================================
\begin{frame}[t,fragile]

\mytitle{Computational Graph and Backpropagation}

\begin{columns}[T,totalwidth=\textwidth]

% --- Left column ---------------------------------------------------------------
\begin{column}[T]{0.52\textwidth}

\textbf{Forward pass}

\begin{itemize}
  \item Input propagated layer by layer
  \item Linear maps + nonlinear activations
  \item Produces model output $\hat{y}$
\end{itemize}

\vspace{1mm}
\textbf{Backward pass}

\begin{itemize}
  \item \y{Loss gradient flows backward}
  \item Chain rule applied automatically
  \item Gradients stored in parameters
\end{itemize}

\vspace{1mm}
Training adjusts parameters using these gradients.

\end{column}

% --- Right column --------------------------------------------------------------
\begin{column}[T]{0.48\textwidth}

\vspace{-8mm}
\begin{minipage}[b]{2cm}
\vspace{1mm}
\centering
Computational graph of a feedforward network
\end{minipage}
\begin{minipage}{4cm}
\hfill\includegraphics[width=0.9\textwidth]{../../images/img05/ffnn_graph.png}
\end{minipage}

\vspace{-2mm}
\raggedright
\textbf{Key idea}

\begin{itemize}
  \item Graph encodes all operations
  \item Enables exact gradient computation
  \item Foundation of learning by optimization
\end{itemize}

\end{column}

\end{columns}

\end{frame}
