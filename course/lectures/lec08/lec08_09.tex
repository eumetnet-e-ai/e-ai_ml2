%!TEX root = lec08.tex
% ================================================================================
% Lecture 08 — Slide 09
% ================================================================================
\begin{frame}[t,fragile]

\mytitle{From Image Tensor to Vision Embedding}

\begin{columns}[T,totalwidth=\textwidth]

% --- Left column ---------------------------------------------------------------
\begin{column}[T]{0.35\textwidth}

\textbf{Vision encoder (ViT)}

\begin{itemize}
  \item Input: \y{image tensor} $(1, 3, 224, 224)$
  \item Image is split into patches
  \item Self-attention models spatial relations
\end{itemize}

\vspace{2mm}
The vision model extracts a \y{high-level representation}
of the entire image.

\end{column}

% --- Right column --------------------------------------------------------------
\begin{column}[T]{0.63\textwidth}

\vspace{-2mm}
\textbf{Forward pass and feature extraction}

\begin{codeonly}{Vision Transformer}
with torch.no_grad():
    vision_out = vit(
        pixel_values=pixel_values )

# Global image representation
vision_feat = (
    vision_out.last_hidden_state[:,0,:]
)  # CLS token
\end{codeonly}

\vspace{1mm}
The \y{CLS token} summarizes the full image content.

\vspace{3mm}
{\footnotesize
$\texttt{img} \in \mathbb{R}^{224\times224\times3}
\;\rightarrow\;
\texttt{vision\_feat} \in \mathbb{R}^{1\times768}$ \\
$\text{CLS token} \in \mathbb{R}^{768}$ — a learned global image representation
}

\end{column}

\end{columns}

\end{frame}
