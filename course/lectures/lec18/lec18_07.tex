%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 â€” Slide 07
% ================================================================================
\begin{frame}[t]

\mytitle{AI-Var: Architecture, Loss, and Learning}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.50\textwidth}
\footnotesize

\textbf{Architecture}

\vspace{1mm}
\begin{itemize}
  \item inputs: \y{background} $x_b$ and \y{observations} $y$
  \item neural network outputs \y{analysis} $\hat{x}_a$
\end{itemize}

\vspace{2mm}
\textbf{Training loss}

\vspace{-3mm}
\[
L
=
(\hat{x}_a-x_b)^T B^{-1}(\hat{x}_a-x_b)
+
(H(\hat{x}_a)-y)^T R^{-1}(H(\hat{x}_a)-y)
\]

\vspace{2mm}
\begin{itemize}
  \item $B^{-1}$ and $R^{-1}$ weight uncertainties
  \item observation operator $H$ is inside the loss
\end{itemize}

\vspace{2mm}
\rtext{\bf Statistics and physics are embedded in the loss.}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.48\textwidth}

\centering
\vspace{-6mm}
\hspace*{0cm}
\begin{minipage}{6cm}
\includegraphics[width=\textwidth]{../../images/img18/aivar1.png}
\end{minipage}

\vspace{2mm}
\footnotesize

\textbf{What is learned}

\vspace{1mm}
\begin{itemize}
  \item mapping $(x_b, y) \;\rightarrow\; x_a$
\end{itemize}

\vspace{1mm}
\textbf{What is \rtext{not} learned}

\vspace{1mm}
\begin{itemize}
  \item no reanalysis targets
  \item no iterative solvers
  \item no adjoint code
\end{itemize}

\vspace{1mm}
\rtext{\bf Optimization is replaced by inference.}

\end{column}

\end{columns}

\end{frame}
