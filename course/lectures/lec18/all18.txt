%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 01
% ================================================================================
\begin{frame}[t]

\mytitle{AI Data Assimilation — Why Does It Matter?}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.52\textwidth}
\footnotesize

\textbf{Numerical Weather Prediction (NWP)}

\vspace{1mm}
Weather forecasts are produced by integrating
\y{high-dimensional dynamical models} forward in time.

\vspace{0mm}
However:

\begin{itemize}
  \item The atmosphere is \y{chaotic}
  \item Small initial errors grow rapidly
  \item Forecast skill is \rtext{\bf dominated by initial conditions}
\end{itemize}

\vspace{0mm}
\textbf{Core problem}

\vspace{1mm}
At any analysis time, we have:
\begin{itemize}
  \item an \y{imperfect model forecast} (background)
  \item \y{sparse, noisy observations}
\end{itemize}

\vspace{1mm}
These must be combined optimally.

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.46\textwidth}
\footnotesize

\textbf{Data assimilation}

\vspace{1mm}
Data assimilation provides a \y{statistically consistent framework}
to merge:
\[
\text{model information}
\quad + \quad
\text{observations}
\]

\vspace{2mm}
The result is the \y{analysis state}:
\begin{itemize}
  \item best estimate of the atmospheric state
  \item starting point for forecasts
\end{itemize}

\vspace{2mm}
\rtext{\bf Data assimilation is the information bottleneck of NWP.}

\vspace{1mm}
Everything that follows — forecasts, warnings, applications —
depends on it.

\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 02
% ================================================================================
\begin{frame}[t]

\mytitle{Classical Data Assimilation: The Analysis Cycle}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.54\textwidth}
\footnotesize

\textbf{The classical DA cycle}

\vspace{1mm}
Operational NWP systems run a \y{repeating assimilation cycle}:

\begin{enumerate}
  \item Start from a \y{background state} $x_b$
  \item Assimilate observations $\;y$
  \item Compute an \y{analysis} $x_a$
  \item Run the numerical model forward
  \item Use the forecast as next background
\end{enumerate}

\vspace{2mm}
This cycle is repeated every few hours as new
observations become available.

\vspace{2mm}
\rtext{\bf The analysis step is the only place where observations enter.}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.48\textwidth}
\footnotesize

\vspace{-2mm}
\textbf{Established DA methods}

\vspace{1mm}
Several algorithmic families are used in practice:

\begin{itemize}
  \item \y{\bf 3D-Var / 4D-Var}
  \begin{itemize}
    \item variational optimization
    \item adjoint-based
  \end{itemize}

  \vspace{0mm}
  \item \y{\bf Ensemble Kalman Filters (EnKF)}
  \begin{itemize}
    \item flow-dependent uncertainty
    \item ensemble statistics
  \end{itemize}

  \vspace{0mm}
  \item \y{\bf Particle Filters}
  \begin{itemize}
    \item fully Bayesian
    \item now also high-dimensional!
  \end{itemize}
\end{itemize}

\vspace{0mm}
All methods aim at the \y{same goal}:
a statistically optimal analysis.

\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 03
% ================================================================================
\begin{frame}[t]

\mytitle{Two AI Paths for Using Observations}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.48\textwidth}
\footnotesize

\textbf{Path 1: AI-based forecasting}

\vspace{1mm}
Observations are used \y{directly inside neural networks}
that produce forecasts.

\vspace{1mm}
Typical characteristics:

\begin{itemize}
  \item observations as additional inputs
  \item sometimes \y{no explicit model state}
  \item learning focuses on \rtext{\bf prediction skill}
\end{itemize}

\vspace{2mm}
This approach bypasses the classical analysis concept.

\vspace{1mm}
\rtext{\bf Observations $\rightarrow$ forecast directly}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.48\textwidth}
\footnotesize

\vspace{-2mm}
\textbf{Path 2: AI-based data assimilation}

\vspace{1mm}
Observations are used to compute an \y{analysis state},
not a forecast.

\vspace{1mm}
Key properties:

\begin{itemize}
  \item preserves the \y{analysis–forecast separation}
  \item consistent with Bayesian DA theory
  \item AI replaces the \y{analysis algorithm}, not the model
\end{itemize}

\vspace{2mm}
\rtext{\bf Observations $\rightarrow$ analysis $\rightarrow$ forecast}

\vspace{1mm}
This is the conceptual space of \y{\bf AI-Var}.

\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 04
% ================================================================================
\begin{frame}[t]

\mytitle{Why Bring AI into Data Assimilation?}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.54\textwidth}
\footnotesize

\textbf{Limits of classical DA}

\vspace{1mm}
State-of-the-art data assimilation systems are:

\begin{itemize}
  \item \y{computationally expensive}
  \item difficult to scale to higher resolution
  \item reliant on \y{adjoint models}
\end{itemize}

\vspace{2mm}
Operational challenges include:

\begin{itemize}
  \item complex model development and maintenance
  \item long wall-clock times
  \item limited flexibility for new observation types
\end{itemize}

\vspace{2mm}
\hspace*{1cm}\begin{minipage}{5cm}
\rtext{\bf DA is often the most expensive component of NWP.}
\end{minipage}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.48\textwidth}
\footnotesize

\vspace{-2mm}
\textbf{What AI can offer}

\vspace{1mm}
Modern neural networks provide:

\begin{itemize}
  \item fast inference once trained
  \item automatic differentiation
  \item flexible nonlinear mappings
\end{itemize}

\vspace{2mm}
Potential benefits for DA:

\begin{itemize}
  \item orders-of-magnitude speedup
  \item end-to-end differentiability
  \item easier adaptation to new data streams
\end{itemize}

\vspace{2mm}
\rtext{\bf Goal: replace the solver, not the statistics.}

\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 05
% ================================================================================
\begin{frame}[t]

\mytitle{From Variational Data Assimilation to AI-Var}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.48\textwidth}
\footnotesize

\textbf{Variational DA: main Idea}

\vspace{3mm}
The analysis is obtained by minimizing:
\[
J(x)
=
\frac{1}{2}(x-x_b)^T B^{-1}(x-x_b)
+
\frac{1}{2}(y-H(x))^T R^{-1}(y-H(x))
\]

\vspace{2mm}
\textbf{Interpretation}

\begin{itemize}
  \item first term: \y{background constraint}
  \item second term: \y{observation constraint}
\end{itemize}

\vspace{2mm}
\rtext{\bf Classical DA = iterative \\numerical minimization.}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.48\textwidth}

\footnotesize
\centering
AI-VAR is the \y{AI verion} of \rtext{\bf 3D-Var, 4D-Var or En-VAR} depending on 
how exactly the minimizer is formulated. 

\vspace{3mm}
\hspace*{-1.3cm}
\begin{minipage}{9cm}
\includegraphics[width=\textwidth]{../../images/img18/aivar2.png}
\end{minipage}

\vspace{1mm}
\footnotesize
AI-Var sits inside the \y{variational DA family},
alongside 3D-Var, 4D-Var, and EnVar.

\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 06
% ================================================================================
\begin{frame}[t]

\mytitle{Core Idea of AI-Var}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.44\textwidth}
\footnotesize

\textbf{Classical variational DA}

\vspace{2mm}
\begin{itemize}
  \item iterative minimization of $J(x)$
  \item requires adjoints and solvers
  \item expensive and sequential
\end{itemize}

\vspace{3mm}
\rtext{\bf Replace the minimization algorithm.}

\vspace{2mm}
\textbf{AI-Var}

\vspace{1mm}
\begin{itemize}
  \item neural network approximates the minimizer
  \item outputs analysis $x_a$ directly
  \item trained using the \y{same cost function}
\end{itemize}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.54\textwidth}

\centering
\vspace{-8mm}
\hspace*{-1cm}
\begin{minipage}{9cm}
\includegraphics[width=\textwidth]{../../images/img18/aivar1.png}
\end{minipage}

\vspace{1mm}
\footnotesize
Classical DA workflow (left) versus AI-based inference of the analysis (right).

\vspace{5mm}
\footnotesize
\rtext{\bf Paradigm shift}

\vspace{1mm}
\begin{itemize}
  \item \color{darkgreen} DA becomes \y{inference}, not optimization
  \item milliseconds instead of iterations
  \item same statistics, different machinery
  \item flexible inflow of further information
\end{itemize}

\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 07
% ================================================================================
\begin{frame}[t]

\mytitle{AI-Var: Architecture, Loss, and Learning}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.50\textwidth}
\footnotesize

\textbf{Architecture}

\vspace{1mm}
\begin{itemize}
  \item inputs: \y{background} $x_b$ and \y{observations} $y$
  \item neural network outputs \y{analysis} $\hat{x}_a$
\end{itemize}

\vspace{2mm}
\textbf{Training loss}

\vspace{-3mm}
\[
L
=
(\hat{x}_a-x_b)^T B^{-1}(\hat{x}_a-x_b)
+
(H(\hat{x}_a)-y)^T R^{-1}(H(\hat{x}_a)-y)
\]

\vspace{2mm}
\begin{itemize}
  \item $B^{-1}$ and $R^{-1}$ weight uncertainties
  \item observation operator $H$ is inside the loss
\end{itemize}

\vspace{2mm}
\rtext{\bf Statistics and physics are embedded in the loss.}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.48\textwidth}

\centering
\vspace{-6mm}
\hspace*{0cm}
\begin{minipage}{6cm}
\includegraphics[width=\textwidth]{../../images/img18/aivar1.png}
\end{minipage}

\vspace{2mm}
\footnotesize

\textbf{What is learned}

\vspace{1mm}
\begin{itemize}
  \item mapping $(x_b, y) \;\rightarrow\; x_a$
\end{itemize}

\vspace{1mm}
\textbf{What is \rtext{not} learned}

\vspace{1mm}
\begin{itemize}
  \item no reanalysis targets
  \item no iterative solvers
  \item no adjoint code
\end{itemize}

\vspace{1mm}
\rtext{\bf Optimization is replaced by inference.}

\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 08
% ================================================================================
\begin{frame}[t, fragile]

\mytitle{1D Data Assimilation Setup}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.46\textwidth}
\footnotesize

\textbf{Toy problem}

\vspace{1mm}
We consider a \y{1D state} on a fixed grid, periodic (!).

\vspace{1mm}
\begin{itemize}
  \item truth: modulated sine function
  \item background $x_b$: shifted, smoothed, biased
  \item observations $y$: sparse, noisy point samples
\end{itemize}

\vspace{2mm}
All ingredients are \y{fully controlled}:
\begin{itemize}
  \item known truth
  \item known error statistics
  \item explicit observation locations
\end{itemize}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.52\textwidth}

\centering
\vspace{-6mm}
\hspace*{0cm}
\begin{minipage}{8cm}
\includegraphics[width=\textwidth]{../../images/img18/1d_xt_xb_obs.png}
\end{minipage}

\vspace{1mm}
\footnotesize
Truth $x_{true}$, background $x_b$, and sparse observations $y$.

\vspace{8mm}
\rtext{\bf Ideal testbed to explore data assimilation.}

\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 09
% ================================================================================
\begin{frame}[t,fragile]

\mytitle{Classical 3D-Var in 1D: Mathematics Made Explicit}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.48\textwidth}
\footnotesize

\textbf{3D-Var cost function}

\vspace{1mm}
The analysis $x_a$ minimizes $J(x)$ from above. 


\vspace{2mm}
\textbf{Linear case (this setup)}

\vspace{1mm}
\begin{itemize}
  \item $H$: point-sampling operator
  \item $B$: Gaussian covariance
  \item $R = \sigma_o^2 I$
\end{itemize}

\vspace{2mm}
\textbf{Closed-form solution}

\vspace{-3mm}
\[
x_a
=
x_b
+
B H^T (H B H^T + R)^{-1}
\bigl(y - H x_b\bigr)
\]

\vspace{2mm}
\rtext{\bf This is the optimal Bayesian estimate.}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.52\textwidth}

\centering
\vspace{-3mm}
\hspace*{0cm}
\begin{minipage}{7cm}
\includegraphics[width=\textwidth]{../../images/img18/1d_xa_3dvar.png}
\end{minipage}

\vspace{1mm}
\footnotesize

Top: state space — truth, background, analysis.  

Bottom: \y{analysis increment}
\[
\delta x = x_a - x_b
\]

\vspace{-2mm}
\y{Information from sparse observations is spread by $B$.}

\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 10
% ================================================================================
\begin{frame}[t,fragile]

\mytitle{AI-Var in 1D: From Mathematics to Code}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.52\textwidth}
\footnotesize

\textbf{Neural increment model}

\vspace{1mm}
The network predicts the \y{analysis increment}:
\[
\delta x_\theta = \mathcal{N}_\theta(x_b, y)
\]

\vspace{1mm}
\textbf{PyTorch implementation (kept simple)}

\tiny
\begin{verbatim}
class IncrementMLP(nn.Module):
    def __init__(self, n):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(3*n, 256),
            nn.Tanh(),
            nn.Linear(256, 256),
            nn.Tanh(),
            nn.Linear(256, n))

    def forward(self, inp):
        return self.net(inp)
\end{verbatim}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.44\textwidth}
\footnotesize

\vspace{-4mm}
{\bf Input vector:}
\[
\texttt{inp} = [x_b,\; y_{\text{grid}},\; \text{mask}]
\]


\textbf{Variational loss in code}

\vspace{1mm}
The 3D-Var cost is used \y{directly}:

\begin{verbatim}
def J_3dvar(delta_x):
    x = xb + delta_x
    innov = y - H @ x

    Jb = 0.5 * (delta_x @ B_inv @ delta_x)
    Jo = 0.5 * (innov @ R_inv @ innov)
    return Jb + Jo
\end{verbatim}

\vspace{2mm}
\rtext{\bf No analysis data.  
No solver.  
Only the variational objective.}

\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 11
% ================================================================================
\begin{frame}[t,fragile]

\mytitle{AI-Var in 1D: Training Loop with Background \& Observations}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.52\textwidth}
\footnotesize

\textbf{Neural input construction}

\vspace{1mm}
The network receives \y{both} background and observations:

\begin{verbatim}
# observations mapped to grid
y_grid = torch.zeros(n)
mask   = torch.zeros(n)

y_grid[obs_idx] = y
mask[obs_idx]   = 1.0

# NN input
inp = torch.cat([xb_t, y_grid, mask])
\end{verbatim}

\vspace{-4mm}
\[
\delta x_\theta
=
\mathcal{N}_\theta(x_b,\; y)
\]

\vspace{0mm}
\hspace*{2cm}\rtext{\bf The solver is replaced by learning.}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.48\textwidth}
\footnotesize

\textbf{Training loop}

\vspace{-2mm}
\begin{verbatim}
for ep in range(n_epochs):
    optimizer.zero_grad()

    delta_x = model(inp)

    loss, Jb, Jo = J_3dvar(delta_x)

    loss.backward()
    optimizer.step()
\end{verbatim}

\vspace{0mm}
\textbf{Key points}

\vspace{1mm}
\begin{itemize}
  \item loss = classical 3D-Var functional
  \item no analysis targets
  \item gradients pass through $B^{-1}$ and $H$
\end{itemize}

\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 12
% ================================================================================
\begin{frame}[t]

\mytitle{1D Result: AI-Var Analysis vs 3D-Var}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.30\textwidth}
\footnotesize

\vspace{-2mm}
\textbf{What is shown}

\vspace{1mm}
\begin{itemize}
  \item true state $x_{true}$
  \item background $x_b$
  \item 3D-Var analysis $x_a$
  \item AI-Var analysis $x_b + \delta x_{\text{ML}}$
\end{itemize}

\vspace{0mm}
\textbf{Bottom panel}

\vspace{1mm}
\begin{itemize}
  \item classical 3D-Var increment
  \item learned AI-Var increment
  \item observation increments at obs points
\end{itemize}

\vspace{0mm}
\rtext{\bf \y{AI-Var reproduces} the \y{variational update.}}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.68\textwidth}

\centering
\vspace{-4mm}
\hspace*{0cm}
\begin{minipage}{10cm}
\includegraphics[width=\textwidth]{../../images/img18/1d_xa_MLP.png}
\end{minipage}

\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 13
% ================================================================================
\begin{frame}[t,fragile]

\mytitle{AI-Var in 1D: Training on Many Cases}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.52\textwidth}
\footnotesize

\textbf{From single case to ensemble training}

\vspace{1mm}
Instead of one fixed $(x_b, y)$ pair, we train on
\y{many randomly generated cases}.

\vspace{2mm}
Each training sample contains:
\begin{itemize}
  \item a new truth $x_{true}$
  \item a new background $x_b$
  \item new observation locations and values $y$
\end{itemize}

\vspace{2mm}
\textbf{What stays fixed}

\vspace{1mm}
\begin{itemize}
  \item background covariance $B$
  \item observation error $R$
  \item variational cost $J$
\end{itemize}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.48\textwidth}
\footnotesize

\vspace{-1mm}
\rtext{\bf The network learns a general DA operator.}

\vspace{2mm}
\textbf{Training logic (conceptual)}

\vspace{1mm}
\begin{verbatim}
for sample in training_set:
    xb, y = sample
    inp = build_input(xb, y)

    delta_x = model(inp)
    loss = J_3dvar(delta_x)

    loss.backward()
    optimizer.step()
\end{verbatim}

\vspace{2mm}
\y{Same loss, many realizations.}

\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 14
% ================================================================================
\begin{frame}[t]

\mytitle{1D Generalization: Many Unseen Test Cases}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.4\textwidth}
\footnotesize

\textbf{Test phase}

\vspace{1mm}
The trained AI-Var network is applied to
\y{previously unseen} cases.

\vspace{2mm}
For each case:
\begin{itemize}
  \item different truth
  \item different background
  \item different observations
\end{itemize}

\vspace{0mm}
\textbf{Observation}

\vspace{1mm}
\begin{itemize}
  \item consistent increments
  \item smooth, physical updates
  \item no retraining required
\end{itemize}

\vspace{1mm}
\rtext{\bf One network, many analyses.}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.55\textwidth}

\centering
\vspace{-3mm}

\includegraphics[width=\textwidth]{../../images/img18/1d_multiple_01_crop.png}
\includegraphics[width=\textwidth]{../../images/img18/1d_multiple_02_crop.png}
\includegraphics[width=\textwidth]{../../images/img18/1d_multiple_05_crop.png}

\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 15
% ================================================================================
\begin{frame}[t, fragile]

\mytitle{2D Data Assimilation Setup}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.34\textwidth}
\footnotesize

\textbf{2D atmospheric toy problem}

\vspace{1mm}
\begin{itemize}
  \item horizontal–vertical grid
  \item structured background errors
  \item sparse column observations
\end{itemize}

\vspace{2mm}
\textbf{State variables}

\vspace{1mm}
\begin{itemize}
  \item truth $x_{true}(x,z)$
  \item background $x_b(x,z)$
  \item analysis $x_a(x,z)$
\end{itemize}

\vspace{2mm}
\rtext{\bf This already resembles NWP geometry.}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.64\textwidth}
\footnotesize

\vspace{-4mm}
\hspace*{-0.5cm}
\begin{minipage}{11cm}
\includegraphics[width=\textwidth]{../../images/img18/2d_setup.png}
\end{minipage}

\begin{minipage}{5cm}
\tiny
\textbf{2D setup in code}

\begin{verbatim}
# 2D grid (x,z)
nx, nz = 120, 40
x = np.linspace(0, Lx, nx)
z = np.linspace(0, Lz, nz)

# background covariance (separable)
B = Bx x Bz

# sparse vertical profiles
H : (n_obs × n_state)
\end{verbatim}
\end{minipage}
\begin{minipage}{3cm}
\includegraphics[width=\textwidth]{../../images/img18/2d_B.png} \\
B matrix in 2d, $B e_{ij}$
\end{minipage}

\vspace{2mm}
\y{Same DA ingredients as in 1D, now on a 2D grid.}


\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 16
% ================================================================================
\begin{frame}[t, fragile]
\begin{tightmath}

\mytitle{2D AI-Var: Explicit Spectral $B^{-1}$ in Flattened Control Space}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.54\textwidth}
\footnotesize

\textbf{2D control vector}

\vspace{0mm}
The 2D field is represented as a \y{1D control vector}:
\[
x \in \mathbb{R}^{n},\qquad n = n_x n_z
\]

\vspace{-1mm}
{\tiny
\begin{verbatim}
nz, nx = xb.shape
n = nz * nx

X_flat = X.reshape(-1)
Z_flat = Z.reshape(-1)
\end{verbatim}
}

\vspace{-1mm}
\textbf{Full Gaussian background covariance}

\vspace{0mm}
{\tiny
\begin{verbatim}
dx2 = (X_flat[:,None]-X_flat[None,:])**2
dz2 = (Z_flat[:,None]-Z_flat[None,:])**2

B = sigma_b**2 * np.exp(
   -0.5*(dx2/Lx**2 + dz2/Lz**2)
)
B = 0.5*(B + B.T)
\end{verbatim}
}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.44\textwidth}
\footnotesize

\textbf{Spectral regularization}

\vspace{0mm}
{\tiny
\begin{verbatim}
lam, U = np.linalg.eigh(B)

lam_floor = alpha * lam.max()
lam_reg   = np.maximum(lam, lam_floor)

Breg_inv = (U * (1.0/lam_reg)) @ U.T
Breg_inv = 0.5*(Breg_inv + Breg_inv.T)
\end{verbatim}
}

\vspace{-1mm}
\textbf{Background term in the loss}

\vspace{-1mm}
\[
J_b(\delta x)
=
\frac12\,\delta x^T\,B^{-1}\,\delta x
\]

\vspace{-1mm}
{\tiny
\begin{verbatim}
Jb = 0.5 * dx @ Breg_inv @ dx
\end{verbatim}
}

\vspace{-1mm}
\rtext{\bf Key point: $B^{-1}$ is built explicitly in this tutorial.}

\end{column}

\end{columns}

\end{tightmath}
\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 17  (reduced)
% ================================================================================
\begin{frame}[t, fragile]
\begin{tightmath}

\mytitle{2D AI-Var (Tutorial Code): Flattened Control + Obs Encoding + 3D-Var Loss}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.52\textwidth}
\footnotesize

\textbf{Inputs: background + obs on grid + mask}

\vspace{0mm}
{\tiny
\begin{verbatim}
# flatten 2D -> 1D control
nz, nx = xb.shape
n = nz * nx
xb_t = torch.tensor(xb.reshape(-1), dtype=dtype, device=device)

# obs indices (iz-major, ix-minor)
obs_indices = iz_idx * nx + ix_idx
y_vec = y_field.reshape(-1)[obs_indices]
y_t   = torch.tensor(y_vec, dtype=dtype, device=device)

# obs on grid + mask (length n)
y_grid = np.zeros(n);  mask = np.zeros(n)
y_grid[obs_indices] = y_vec
mask[obs_indices]   = 1.0

inp_t = torch.cat([xb_t,
       torch.tensor(y_grid, dtype=dtype, device=device),
       torch.tensor(mask,   dtype=dtype, device=device)], dim=0)
\end{verbatim}
}

\vspace{-1mm}
\rtext{\bf Input is $[x_b,\; y_{\rm grid},\; {\rm mask}]$ in control space.}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.46\textwidth}
\footnotesize

\textbf{Increment MLP + variational loss}

\vspace{0mm}
{\tiny
\begin{verbatim}
class IncrementMLP(nn.Module):
    def __init__(self, n):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(3*n, 256), nn.Tanh(),
            nn.Linear(256, 256), nn.Tanh(),
            nn.Linear(256, n),
        )

def quadform(A, v): return torch.dot(v, A @ v)

def J_3dvar(dx):
    x = xb_t + dx
    innov = y_t - (H_t @ x)
    Jb = 0.5 * quadform(B_inv_t, dx)
    Jo = 0.5 * quadform(R_inv_t, innov)
    return Jb + Jo
\end{verbatim}
}

\vspace{-1mm}
\y{Exactly the classical 3D-Var objective,} \\
\y{now differentiable.}

\end{column}

\end{columns}

\end{tightmath}
\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 18
% ================================================================================
\begin{frame}[t, fragile]

\mytitle{2D Result: Variational Reference vs Learned Analysis}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.35\textwidth}
\footnotesize

\textbf{What is compared}

\vspace{1mm}
\begin{itemize}
  \item background $x_b$
  \item 3D-Var analysis $x_a$
  \item AI-Var analysis $\hat{x}_a$
\end{itemize}

\vspace{2mm}
\textbf{Key observation}

\vspace{1mm}
\begin{itemize}
  \item smooth spatial increments
  \item correct information spreading
  \item close to variational solution
\end{itemize}

\vspace{2mm}
\rtext{\bf Learned analysis = inference of the minimizer.}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.63\textwidth}

\centering
\vspace{-3mm}
\hspace*{0cm}
\begin{minipage}{8.5cm}
\includegraphics[width=\textwidth]{../../images/img18/2d_xa_MLP_xa_3dVAR_crop.png}
\end{minipage}

\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 19  (tight version)
% ================================================================================
\begin{frame}[t]
\begin{tightmath}

\mytitle{AI Particle Filter (Gaussian Mixture): Core Idea}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.54\textwidth}
\footnotesize

\textbf{Filtering problem}

\vspace{0mm}
Sequential posterior:

\vspace{-3mm}
\[
p(x_n \mid y_{1:n})
\]

\vspace{-1mm}
Represent the distribution by \y{particles}

\vspace{-2mm}
\[
X_n = \{x^{(i)}_n\}_{i=1}^N
\]

\vspace{1mm}
\textbf{AI Particle Filter (this tutorial)}

\vspace{0mm}
Forecast ensemble $X^b_n$ is transformed into analysis ensemble:
\[
X^a_n = \mathcal{N}_\theta(X^b_n,\; y_n)
\]

\vspace{1mm}
\textbf{Gaussian mixture view}

\vspace{0mm}
Both prior and posterior are approximated by mixtures:
\[
q^b(x)\approx \frac1N\sum_i \mathcal{N}(x|x^{b,(i)},\Sigma),
\qquad
q^a(x)\approx \frac1N\sum_i \mathcal{N}(x|x^{a,(i)},\Sigma)
\]

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.42\textwidth}
\footnotesize

\textbf{Model problem: Lorenz-63}

\vspace{0mm}
\begin{itemize}
  \item state $x=(x_1,x_2,x_3)\in\mathbb{R}^3$
  \item partial observations $y=(x_1,x_2)$ + noise
  \item wrong forecast model (intentional)
\end{itemize}

\vspace{1mm}
\textbf{Key message}

\vspace{0mm}
Instead of resampling / MCMC moves, we learn a
\y{distribution transform} that fits the ensemble to the posterior.

\vspace{4mm}
\rtext{\bf Network output is a posterior particle cloud.}

\end{column}

\end{columns}

\end{tightmath}
\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 20
% ================================================================================
\begin{frame}[t, fragile]
\begin{tightmath}

\mytitle{Neural Particle Update: DeepSets (Permutation Invariance)}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.50\textwidth}
\footnotesize

\textbf{Particle set input}

\vspace{0mm}
Forecast ensemble is an unordered set:
\[
X^b = \{x_i^b\}_{i=1}^N
\]

\vspace{1mm}
Update must be \y{permutation invariant}:
\[
\mathcal{N}_\theta(\pi X^b,\;y)=\pi\,\mathcal{N}_\theta(X^b,\;y)
\]

\vspace{2mm}
\textbf{DeepSets structure}

\vspace{0mm}
\[
\phi(x_i,y)\;\rightarrow\;
\text{mean pool}\;\rightarrow\;
\rho(\cdot)\;\rightarrow\;
\psi(x_i,\text{context},y)
\]

\vspace{1mm}
\rtext{\bf Each particle sees local state + global context + obs.}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.46\textwidth}
\footnotesize

{\tiny
\begin{verbatim}
class ParticleUpdateNN(nn.Module):
  def forward(self, Xb, y):
    N = Xb.shape[0]
    y_rep = y.expand(N, -1)

    emb = phi(torch.cat([Xb, y_rep], 1))
    pooled = emb.mean(0, keepdim=True)
    ctx = rho(pooled).expand(N, -1)

    dX = psi(torch.cat([Xb, ctx, y_rep], 1))
    return Xb + dX
\end{verbatim}
}

\vspace{-1mm}
\y{Output: updated ensemble $X^a$.}

\end{column}

\end{columns}

\end{tightmath}
\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 21
% ================================================================================
\begin{frame}[t, fragile]
\begin{tightmath}

\mytitle{Training Loss: Fit a Gaussian-Mixture Posterior}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.56\textwidth}
\footnotesize

\textbf{Posterior fitting objective}

\vspace{0mm}
Train the particle update network such that
the \y{analysis ensemble} represents the posterior:
\[
q^a(x)\;\approx\; p(x\mid y)
\]

\vspace{1mm}
\textbf{Gaussian mixture model}

\vspace{-2mm}
Particles define a mixture density:
\[
q(x\mid X)=\frac1N\sum_{i=1}^N\mathcal{N}(x\mid x^{(i)},\Sigma)
\]

\vspace{-1mm}
\textbf{Loss = likelihood + KL fit}

\vspace{-4mm}
\[
L
=
\underbrace{-\log\Big(\tfrac1N\sum_i p(y\mid x_i^a)\Big)}_{L_{\rm obs}}
+
\lambda_{\rm bg}\,
\underbrace{\mathrm{KL}\!\Big(q_{\rm target}(\cdot)\,\Vert\,q^a(\cdot)\Big)}_{L_{\rm GM}}
\]

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.40\textwidth}
\footnotesize

\vspace{-3mm}
{\tiny
\begin{verbatim}
# evaluation points (single Kalman update)
Z = kalman_eval_points(Xb, y)

# target posterior weights on Z
log_t = log_mix(Z|Xb) + log_like(y|Z)
w_t = softmax(log_t)

# model mixture log-density on Z
log_m = log_mix(Z|Xa)

# mixture KL term
L_GM = sum_z w_t(z) * (log w_t(z) 
	- log softmax(log_m(z)))
loss = L_obs + lambda_bg * L_GM
\end{verbatim}
}

\vspace{-1mm}
\y{Target uses prior mixture + obs likelihood.}

\vspace{4mm}
\centering
\rtext{\bf Learn the posterior distribution, not only the mean.}

\end{column}

\end{columns}

\end{tightmath}
\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 22
% ================================================================================
\begin{frame}[t, fragile]
\begin{tightmath}

\mytitle{Gaussian-Mixture PF: Comparing Two Distributions (Math)}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.52\textwidth}
\footnotesize

\textbf{1) Ensembles as Gaussian mixtures}

\vspace{0mm}
Forecast particles $X^b=\{x_i^b\}_{i=1}^N$ and analysis particles
$X^a=\{x_i^a\}_{i=1}^N$ define mixture densities, e.g.:
\vspace{-2mm}
\begin{equation*}
q^a_\theta(x)=\tfrac1N\sum_{i=1}^N \mathcal{N}(x\mid x_i^a,\Sigma)
\end{equation*}

\vspace{-1mm}
\textbf{2) Discretize comparison}

\vspace{0mm}
Choose evaluation points $Z=\{z_k\}_{k=1}^K$
(from a Kalman-type proposal step in the notebook).

\textbf{Target posterior on $Z$}

\vspace{-2mm}
\begin{equation*}
w^\star_k
=
\frac{q^b(z_k)\;p(y\mid z_k)}
     {\sum_{\ell=1}^K q^b(z_\ell)\;p(y\mid z_\ell)}
\end{equation*}

\vspace{-1mm}
\y{Prior mixture + obs likelihood $\Rightarrow$ posterior weights.}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.46\textwidth}
\footnotesize

\textbf{3) Model distribution on $Z$}

\vspace{0mm}
Evaluate the analysis mixture on the same points and normalize:
\vspace{-2mm}
\begin{equation*}
\pi_{\theta,k}
=
\frac{q^a_\theta(z_k)}
     {\sum_{\ell=1}^K q^a_\theta(z_\ell)}
=
\mathrm{softmax}\!\big(\log q^a_\theta(z_k)\big)
\end{equation*}

\vspace{2mm}
\textbf{4) Fit posterior mass distribution}

\vspace{0mm}
Training minimizes the discrete KL divergence:
\vspace{-2mm}
\begin{equation*}
L_{\rm GM}
=
\mathrm{KL}(w^\star\;\|\;\pi_\theta)
=
\sum_{k=1}^K w^\star_k\;
\log\frac{w^\star_k}{\pi_{\theta,k}}
\end{equation*}

\vspace{-1mm}
\rtext{\bf Network learns to move particles toward posterior mass.}

\end{column}

\end{columns}

\end{tightmath}
\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 23
% ================================================================================
\begin{frame}[t]

\mytitle{Ensemble Geometry: Prior $\rightarrow$ Analysis in a 2D Slice}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.30\textwidth}
\footnotesize

\vspace{-2mm}
\textbf{What the plot shows}

\vspace{0mm}
2D slice of the state space (e.g. $x_1$--$x_2$ plane).

\vspace{2mm}
\textbf{Prior (forecast ensemble)}

\vspace{0mm}
\begin{itemize}
  \item particles $X^b$ (cloud)
  \item prior mean $\bar{x}^b$
\end{itemize}

\vspace{1mm}
\textbf{Observation / truth}

\vspace{0mm}
\begin{itemize}
  \item observation $y$ (marker)
  \item truth $x_{\rm true}$ (marker)
\end{itemize}

\vspace{1mm}
\textbf{Analysis ensemble}

\vspace{0mm}
\begin{itemize}
  \item updated particles $X^a$
  \item analysis mean $\bar{x}^a$
\end{itemize}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.68\textwidth}

\centering
\vspace{-4mm}
\hspace*{-0.5cm}
\begin{minipage}{10cm}
\includegraphics[width=\textwidth]{../../images/img18/f_ensemble_scatter_step_0022.png}
\end{minipage}

\vspace{-2mm}
\tiny
Prior ensemble (forecast) and analysis ensemble after the 
learned update.

\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 24
% ================================================================================
\begin{frame}[t]

\mytitle{Ablation: Why the Background / Posterior-Fit Term Matters}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.30\textwidth}
\footnotesize

\textbf{Two trained networks}

\vspace{0mm}
\begin{itemize}
  \item \y{\bf net}: full AI-PF loss
  \item \y{\bf net\_obs}: obs term only
\end{itemize}

\vspace{2mm}
\textbf{Metric shown}

\vspace{0mm}
Difference of first-guess error:
\[
\Delta
=
{\rm FG}_{\rm err}(\texttt{net\_obs})
-
{\rm FG}_{\rm err}(\texttt{net})
\]

\vspace{2mm}
\textbf{Interpretation}

\vspace{0mm}
\begin{itemize}
  \item \y{$\Delta>0$ (green)}: net is better
  \item $\Delta<0$ (red): net\_obs is better
\end{itemize}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.68\textwidth}

\centering
\vspace{-4mm}
\hspace*{0cm}
\begin{minipage}{9cm}
\includegraphics[width=\textwidth]{../../images/img18/fg_error_diff_netobs_minus_net.png}
\end{minipage}

\vspace{1mm}
\footnotesize
Evaluation over 10\,000 assimilation cycles after short training.

\includegraphics[width=5cm]{../../images/img18/fg_error_diff_10000.png}



\end{column}

\end{columns}

\end{frame}
%!TEX root = lec18.tex
% ================================================================================
% Lecture 18 — Slide 25
% ================================================================================
\begin{frame}[t]

\mytitle{Summary: Two AI Paths for Data Assimilation}

\begin{columns}[T,totalwidth=\textwidth]

% ------------------------------------------------------------
\begin{column}[T]{0.48\textwidth}
\footnotesize

\textbf{AI-Var (Keller \& Potthast)}

\y{\bf What it learns:}

\vspace{-3mm}
\[
(x_b,\;y)\;\mapsto\; x_a
\]

\vspace{0mm}
\begin{itemize}
  \item neural network approximates the minimizer
  \item trained by variational cost $J(x)$
  \item output = \y{one deterministic analysis field}
\end{itemize}

\vspace{2mm}
\y{\bf Strengths}
\begin{itemize}
  \item stable, structured increments
  \item scalable in dimension (1D $\rightarrow$ 2D demo)
  \item direct link to operational 3D/4D-Var
\end{itemize}

\end{column}

% ------------------------------------------------------------
\begin{column}[T]{0.48\textwidth}
\footnotesize

\vspace{-3mm}
\textbf{AI Particle Filter (Gaussian mixture)}

\y{\bf What it learns:}

\vspace{-2mm}
\[
(X^b,\;y)\;\mapsto\; X^a
\]

\vspace{0mm}
\begin{itemize}
  \item neural update transforms particle cloud
  \item trained to fit posterior mass distribution
  \item output = \y{analysis distribution (ensemble)}
\end{itemize}

\vspace{2mm}
\y{\bf Strengths}
\begin{itemize}
  \item non-Gaussian posteriors (multi-modal)
  \item distribution-aware filtering
  \item background-term ablation shows skill gain
\end{itemize}

\end{column}

\end{columns}

\vspace{4mm}
\rtext{\bf Take-home: AI-Var learns the analysis; AI-PF learns a posterior distribution.}

\end{frame}
% ================================================================================
% E-AI Tutorial Slides
% Filename: lec18.tex
%
% Roland Potthast 2025/2026
% Licence: CC-BY4.0
% ================================================================================
\documentclass[aspectratio=169]{beamer}

% --- Load lecture macros --------------------------------------------------------
\input{../lec_macros.tex}
\newcommand{\LectureNumber}{Lecture 18}

% --- Document -------------------------------------------------------------------
\begin{document}

\setagendaboxforlecture{18}

\input{../lec_agenda.tex}
\input{lec18_01.tex}
\input{lec18_02.tex}
\input{lec18_03.tex}
\input{lec18_04.tex}
\input{lec18_05.tex}
\input{lec18_06.tex}
\input{lec18_07.tex}
\input{lec18_08.tex}
\input{lec18_09.tex}
\input{lec18_10.tex}
\input{lec18_11.tex}
\input{lec18_12.tex}
\input{lec18_13.tex}
\input{lec18_14.tex}
\input{lec18_15.tex}
\input{lec18_16.tex}
\input{lec18_17.tex}
\input{lec18_18.tex}
\input{lec18_19.tex}
\input{lec18_20.tex}
\input{lec18_21.tex}
\input{lec18_22.tex}
\input{lec18_23.tex}
\input{lec18_24.tex}
\input{lec18_25.tex}

% --- End Document ---------------------------------------------------------------
\end{document}
