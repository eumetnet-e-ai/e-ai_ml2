%!TEX root = lec06.tex
% ================================================================================
% Lecture 6 â€” Slide 16
% ================================================================================
\begin{frame}[t,fragile]

\mytitle{Output Projection to Vocabulary}

\begin{columns}[T,totalwidth=\textwidth]

% --- Left column ---------------------------------------------------------------
\begin{column}[T]{0.48\textwidth}

\vspace{-1mm}
\textbf{Final Transformer output}

After all Transformer blocks:
\[
Z \in \mathbb{R}^{n \times d_{\text{model}}}
\]

\vspace{1mm}
Each row corresponds to:
\begin{itemize}
  \item One token position
  \item A contextualized representation
\end{itemize}

\vspace{1mm}
\textbf{Goal}

\begin{itemize}
  \item Predict the next token
  \item From a fixed vocabulary
\end{itemize}

\end{column}

% --- Right column --------------------------------------------------------------
\begin{column}[T]{0.5\textwidth}

\vspace{-9mm}
\textbf{Linear projection to logits}

\[
Z_{\text{logits}}
=
Z W_{\text{out}} + b_{\text{out}}
\]

\begin{itemize}
  \item $W_{\text{out}} \in \mathbb{R}^{d_{\text{model}} \times V}$
  \item $b_{\text{out}} \in \mathbb{R}^{V}$
  \item $V$: \y{vocabulary size}
\end{itemize}

\vspace{1mm}
\textbf{Softmax probabilities}

\[
Z_{\text{pred}}
=
\operatorname{softmax}(Z_{\text{logits}})
\]

\vspace{1mm}
\y{Each row is a probability distribution}
\y{over tokens.}

\end{column}

\end{columns}

\vspace{2mm}
\centering
{\footnotesize
\rtext{Logits convert hidden representations into token probabilities.}
}

\end{frame}
