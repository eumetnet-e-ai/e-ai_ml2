
\mytitle{Accessing Ollama via a Local Streaming Server}

\begin{columns}[T,totalwidth=\textwidth]

% --- Left column ---------------------------------------------------------------
\begin{column}[T]{0.35\textwidth}

\vspace{-1mm}
\textbf{Architecture overview}

\begin{itemize}
  \item Ollama runs the LLM locally
  \item Flask provides a lightweight API
  \item HTML/JS frontend streams responses
\end{itemize}

\end{column}

% --- Right column --------------------------------------------------------------
\begin{column}[T]{0.63\textwidth}

\vspace{-1mm}
\textbf{Starting the streaming server}

\begin{codeonly}{Run the Flask server}
cd code06
python 5_flask_streaming.py
\end{codeonly}

\vspace{1mm}
\textbf{Open the UI in your browser}

\begin{codeonly}{Access the interface}
http://127.0.0.1:5000
\end{codeonly}


\vspace{0mm}
{\footnotesize
Responses are streamed token by token via
\texttt{text/event-stream}.
}

\end{column}

\end{columns}

\vspace{-2mm}
{\color{red}
$ \textbf{Browser}
\;\rightarrow\;
\textbf{Flask server}
\;\rightarrow\;
\textbf{Ollama}
$}

\vspace{1mm}
{\footnotesize
No cloud, no external services, full local control.
}

\vspace{1mm}
\y{This setup turns a local LLM into an interactive application platform.}

